{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUz4e3G-ezSa",
        "outputId": "c594385e-be88-4c9c-d252-7f441013450c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Model Comparison and Evaluation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "print(\"Evaluation setup complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load predictions from all models (you'll add these as team members complete)\n",
        "# For now, we have your Dense model predictions\n",
        "\n",
        "# Dense Model (YOURS - already exists)\n",
        "dense_results = pd.read_csv('dense_model_predictions.csv')\n",
        "print(\"Dense model predictions loaded\")\n",
        "\n",
        "# Create placeholders for other models (team will add these)\n",
        "try:\n",
        "    lstm_results = pd.read_csv('lstm_model_predictions.csv')\n",
        "    print(\"LSTM model predictions loaded\")\n",
        "except:\n",
        "    lstm_results = None\n",
        "    print(\"LSTM predictions not available yet\")\n",
        "\n",
        "try:\n",
        "    cnn_results = pd.read_csv('cnn_model_predictions.csv')\n",
        "    print(\"CNN model predictions loaded\")\n",
        "except:\n",
        "    cnn_results = None\n",
        "    print(\" CNN predictions not available yet\")\n",
        "\n",
        "try:\n",
        "    gru_results = pd.read_csv('gru_model_predictions.csv')\n",
        "    print(\" GRU model predictions loaded\")\n",
        "except:\n",
        "    gru_results = None\n",
        "    print(\" GRU predictions not available yet\")\n",
        "\n",
        "print(f\"\\n Dense model samples: {len(dense_results)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "5UgyYhSVfUZo",
        "outputId": "0eb98ed2-73d7-4163-990f-6a6269fb1c71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dense_model_predictions.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2595388294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Dense Model (YOURS - already exists)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdense_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dense_model_predictions.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Dense model predictions loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dense_model_predictions.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate all metrics\n",
        "def calculate_metrics(actual, predicted, model_name):\n",
        "    mae = mean_absolute_error(actual, predicted)\n",
        "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
        "    r2 = r2_score(actual, predicted)\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'MAE': round(mae, 4),\n",
        "        'RMSE': round(rmse, 4),\n",
        "        'R²': round(r2, 4)\n",
        "    }\n",
        "\n",
        "# Calculate metrics for available models\n",
        "metrics_data = []\n",
        "\n",
        "# Dense Model\n",
        "metrics_data.append(calculate_metrics(\n",
        "    dense_results['actual'],\n",
        "    dense_results['predicted'],\n",
        "    'Dense'\n",
        "))\n",
        "\n",
        "# LSTM Model\n",
        "if lstm_results is not None:\n",
        "    metrics_data.append(calculate_metrics(\n",
        "        lstm_results['actual'],\n",
        "        lstm_results['predicted'],\n",
        "        'LSTM'\n",
        "    ))\n",
        "\n",
        "# CNN Model\n",
        "if cnn_results is not None:\n",
        "    metrics_data.append(calculate_metrics(\n",
        "        cnn_results['actual'],\n",
        "        cnn_results['predicted'],\n",
        "        'CNN'\n",
        "    ))\n",
        "\n",
        "# GRU Model\n",
        "if gru_results is not None:\n",
        "    metrics_data.append(calculate_metrics(\n",
        "        gru_results['actual'],\n",
        "        gru_results['predicted'],\n",
        "        'GRU'\n",
        "    ))\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame(metrics_data)\n",
        "print(\"MODEL COMPARISON METRICS:\")\n",
        "print(comparison_df)"
      ],
      "metadata": {
        "id": "5ZYGU-2XfYq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bar chart comparison of metrics\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# MAE Comparison\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.bar(comparison_df['Model'], comparison_df['MAE'], color=['blue', 'orange', 'green', 'red'][:len(comparison_df)])\n",
        "plt.title('MAE Comparison (Lower is Better)', fontweight='bold')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# RMSE Comparison\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.bar(comparison_df['Model'], comparison_df['RMSE'], color=['blue', 'orange', 'green', 'red'][:len(comparison_df)])\n",
        "plt.title('RMSE Comparison (Lower is Better)', fontweight='bold')\n",
        "plt.ylabel('Root Mean Squared Error')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# R² Comparison\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.bar(comparison_df['Model'], comparison_df['R²'], color=['blue', 'orange', 'green', 'red'][:len(comparison_df)])\n",
        "plt.title('R² Comparison (Higher is Better)', fontweight='bold')\n",
        "plt.ylabel('R-squared Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fCH2-zFgfaaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot actual vs predictions for all models\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot 1: All models predictions\n",
        "plt.subplot(2, 1, 1)\n",
        "sample_range = range(min(30, len(dense_results)))\n",
        "\n",
        "# Plot actual values\n",
        "plt.plot(sample_range, dense_results['actual'].values[:30],\n",
        "         label='Actual', color='black', linewidth=3, marker='o')\n",
        "\n",
        "# Plot model predictions\n",
        "plt.plot(sample_range, dense_results['predicted'].values[:30],\n",
        "         label='Dense', alpha=0.8, marker='s')\n",
        "\n",
        "if lstm_results is not None:\n",
        "    plt.plot(sample_range, lstm_results['predicted'].values[:30],\n",
        "             label='LSTM', alpha=0.8, marker='^')\n",
        "\n",
        "if cnn_results is not None:\n",
        "    plt.plot(sample_range, cnn_results['predicted'].values[:30],\n",
        "             label='CNN', alpha=0.8, marker='d')\n",
        "\n",
        "if gru_results is not None:\n",
        "    plt.plot(sample_range, gru_results['predicted'].values[:30],\n",
        "             label='GRU', alpha=0.8, marker='v')\n",
        "\n",
        "plt.title('Model Predictions Comparison (First 30 Samples)', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Test Samples')\n",
        "plt.ylabel('Capacity')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Error distribution\n",
        "plt.subplot(2, 1, 2)\n",
        "errors_data = []\n",
        "\n",
        "# Calculate errors\n",
        "dense_errors = dense_results['predicted'] - dense_results['actual']\n",
        "errors_data.append(dense_errors)\n",
        "\n",
        "if lstm_results is not None:\n",
        "    lstm_errors = lstm_results['predicted'] - lstm_results['actual']\n",
        "    errors_data.append(lstm_errors)\n",
        "\n",
        "if cnn_results is not None:\n",
        "    cnn_errors = cnn_results['predicted'] - cnn_results['actual']\n",
        "    errors_data.append(cnn_errors)\n",
        "\n",
        "if gru_results is not None:\n",
        "    gru_errors = gru_results['predicted'] - gru_results['actual']\n",
        "    errors_data.append(gru_errors)\n",
        "\n",
        "model_names = ['Dense']\n",
        "if lstm_results is not None: model_names.append('LSTM')\n",
        "if cnn_results is not None: model_names.append('CNN')\n",
        "if gru_results is not None: model_names.append('GRU')\n",
        "\n",
        "plt.boxplot(errors_data, labels=model_names)\n",
        "plt.title('Prediction Error Distribution', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('Prediction Error (Predicted - Actual)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "poT4QY3gfcLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank models by performance\n",
        "print(\"MODEL RANKINGS:\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Rank by MAE (primary metric)\n",
        "mae_ranking = comparison_df.sort_values('MAE')\n",
        "print(\"1. By MAE (Primary Metric - Lower is Better):\")\n",
        "for i, (_, row) in enumerate(mae_ranking.iterrows(), 1):\n",
        "    print(f\"   {i}. {row['Model']}: MAE = {row['MAE']}\")\n",
        "\n",
        "print(\"\\n2. By R² (Secondary Metric - Higher is Better):\")\n",
        "r2_ranking = comparison_df.sort_values('R²', ascending=False)\n",
        "for i, (_, row) in enumerate(r2_ranking.iterrows(), 1):\n",
        "    print(f\"   {i}. {row['Model']}: R² = {row['R²']}\")\n",
        "\n",
        "print(\"\\n3. Overall Performance Summary:\")\n",
        "best_model_mae = mae_ranking.iloc[0]\n",
        "print(f\" BEST MODEL: {best_model_mae['Model']}\")\n",
        "print(f\"   - MAE: {best_model_mae['MAE']}\")\n",
        "print(f\"   - RMSE: {best_model_mae['RMSE']}\")\n",
        "print(f\"   - R²: {best_model_mae['R²']}\")\n",
        "\n",
        "# Calculate improvement over baseline (Dense)\n",
        "if len(comparison_df) > 1:\n",
        "    dense_mae = comparison_df[comparison_df['Model'] == 'Dense']['MAE'].values[0]\n",
        "    best_mae = best_model_mae['MAE']\n",
        "    improvement = ((dense_mae - best_mae) / dense_mae) * 100\n",
        "    print(f\" Improvement over Dense baseline: {improvement:.1f}%\")"
      ],
      "metadata": {
        "id": "Y5SAvUhKfezf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save comparison results\n",
        "comparison_df.to_csv('model_comparison_results.csv', index=False)\n",
        "\n",
        "# Create detailed report\n",
        "report = f\"\"\"\n",
        "MODEL COMPARISON REPORT\n",
        "Generated on: {pd.Timestamp.now()}\n",
        "\n",
        "BEST PERFORMING MODEL: {best_model_mae['Model']}\n",
        "- MAE: {best_model_mae['MAE']}\n",
        "- RMSE: {best_model_mae['RMSE']}\n",
        "- R²: {best_model_mae['R²']}\n",
        "\n",
        "FULL COMPARISON:\n",
        "{comparison_df.to_string()}\n",
        "\n",
        "CONCLUSION:\n",
        "The {best_model_mae['Model']} model demonstrated the best performance for battery capacity prediction, achieving the lowest MAE and highest R² score among all tested architectures.\n",
        "\"\"\"\n",
        "\n",
        "with open('model_comparison_report.txt', 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(\"Comparison results saved!\")\n",
        "print(\"Files created:\")\n",
        "print(\"   - model_comparison_results.csv\")\n",
        "print(\"   - model_comparison_report.txt\")\n",
        "print(\"\\n EVALUATION COMPLETE! Ready for report and presentation!\")"
      ],
      "metadata": {
        "id": "S3EAtfC_fhQa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}